# Backend API Documentation

## üéØ Overview

Ce backend FastAPI expose une **API compatible Ollama** qui traduit les requ√™tes vers Mistral AI.

## üîó Endpoints

### Health Check

```http
GET /
```

**Response:**
```json
{
  "status": "ok",
  "service": "French Sovereign Chatbot Backend (Ollama-compatible)",
  "mistral_api_configured": true
}
```

---

### List Models (Ollama-compatible)

```http
GET /api/tags
```

Liste les mod√®les disponibles au format Ollama.

**Response:**
```json
{
  "models": [
    {
      "name": "mistral-large",
      "model": "mistral-large",
      "modified_at": "2024-01-01T00:00:00Z",
      "size": 123456789,
      "digest": "sha256:abc123",
      "details": {
        "parent_model": "",
        "format": "mistral",
        "family": "mistral",
        "families": ["mistral"],
        "parameter_size": "70B",
        "quantization_level": "Q4_0"
      }
    },
    {
      "name": "mistral-small",
      "model": "mistral-small",
      "modified_at": "2024-01-01T00:00:00Z",
      "size": 45678901,
      "digest": "sha256:def456",
      "details": {
        "parent_model": "",
        "format": "mistral",
        "family": "mistral",
        "families": ["mistral"],
        "parameter_size": "7B",
        "quantization_level": "Q4_0"
      }
    }
  ]
}
```

---

### Generate (Ollama-compatible)

```http
POST /api/generate
```

G√©n√©ration de texte √† partir d'un prompt (streaming ou non).

**Request Body:**
```json
{
  "model": "mistral-large",
  "prompt": "Explique-moi la souverainet√© num√©rique",
  "stream": true,
  "temperature": 0.7,
  "max_tokens": 4096
}
```

**Response (streaming, NDJSON):**
```json
{"model":"mistral-large","created_at":"2024-01-01T12:00:00Z","response":"La souverainet√©","done":false}
{"model":"mistral-large","created_at":"2024-01-01T12:00:00Z","response":" num√©rique","done":false}
{"model":"mistral-large","created_at":"2024-01-01T12:00:00Z","response":"...","done":false}
{"model":"mistral-large","created_at":"2024-01-01T12:00:00Z","response":"","done":true}
```

**Response (non-streaming):**
```json
{
  "model": "mistral-large",
  "created_at": "2024-01-01T12:00:00Z",
  "response": "La souverainet√© num√©rique est...",
  "done": true
}
```

---

### Chat (Ollama-compatible)

```http
POST /api/chat
```

Chat conversationnel avec historique (streaming ou non).

**Request Body:**
```json
{
  "model": "mistral-large",
  "messages": [
    {
      "role": "system",
      "content": "Tu es un assistant sp√©cialis√© en souverainet√© num√©rique."
    },
    {
      "role": "user",
      "content": "Pourquoi Mistral AI ?"
    }
  ],
  "stream": true,
  "temperature": 0.7,
  "max_tokens": 4096
}
```

**Response (streaming, NDJSON):**
```json
{"model":"mistral-large","created_at":"2024-01-01T12:00:00Z","message":{"role":"assistant","content":"Mistral"},"done":false}
{"model":"mistral-large","created_at":"2024-01-01T12:00:00Z","message":{"role":"assistant","content":" AI"},"done":false}
{"model":"mistral-large","created_at":"2024-01-01T12:00:00Z","message":{"role":"assistant","content":"..."},"done":false}
{"model":"mistral-large","created_at":"2024-01-01T12:00:00Z","message":{"role":"assistant","content":""},"done":true}
```

**Response (non-streaming):**
```json
{
  "model": "mistral-large",
  "created_at": "2024-01-01T12:00:00Z",
  "message": {
    "role": "assistant",
    "content": "Mistral AI est un fournisseur fran√ßais..."
  },
  "done": true
}
```

---

### Pull Model (Mock)

```http
POST /api/pull
```

Mock endpoint pour la compatibilit√© Ollama. Retourne succ√®s imm√©diat (pas de t√©l√©chargement r√©el).

**Request Body:**
```json
{
  "name": "mistral-large"
}
```

**Response:**
```json
{
  "status": "success",
  "digest": "sha256:abc123",
  "total": 1000000,
  "completed": 1000000
}
```

---

## üîÑ Translation Flow

### Ollama Request ‚Üí Mistral API

Le backend traduit automatiquement :

| Ollama Format | Mistral API Format |
|--------------|-------------------|
| `model: "mistral-large"` | `model: "mistral-large-latest"` |
| `model: "mistral-small"` | `model: "mistral-small-latest"` |
| `prompt: "text"` | `messages: [{"role": "user", "content": "text"}]` |
| NDJSON streaming | SSE streaming |

### Mistral API Response ‚Üí Ollama Format

Streaming SSE de Mistral :
```
data: {"choices":[{"delta":{"content":"Hello"}}]}
data: [DONE]
```

Devient NDJSON Ollama :
```json
{"model":"mistral-large","response":"Hello","done":false}
{"model":"mistral-large","response":"","done":true}
```

---

## üîê Authentication

Le backend utilise `MISTRAL_API_KEY` depuis les variables d'environnement :

```bash
MISTRAL_API_KEY=votre_cle_api
```

Pas d'authentification expos√©e au frontend (le backend g√®re tout).

---

## üö® Error Handling

### 500 - API Key Missing
```json
{
  "detail": "MISTRAL_API_KEY not configured"
}
```

### 504 - Timeout
```json
{
  "detail": "Mistral API timeout"
}
```

### 500 - Mistral API Error
```json
{
  "detail": "Mistral API error: <error_message>"
}
```

---

## üß™ Testing

### Test Health Check
```bash
curl http://localhost:8000/
```

### Test Models List
```bash
curl http://localhost:8000/api/tags
```

### Test Chat (streaming)
```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral-large",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "stream": true
  }'
```

### Test Generate (non-streaming)
```bash
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral-large",
    "prompt": "Explique-moi Docker",
    "stream": false
  }'
```

---

## üìñ Interactive Docs

FastAPI g√©n√®re automatiquement de la documentation interactive :

- **Swagger UI** : http://localhost:8000/docs
- **ReDoc** : http://localhost:8000/redoc

---

## üîß CORS Configuration

Le backend autorise les requ√™tes depuis :
- `http://localhost:3000` (dev)
- `http://localhost:3001`
- Variable `FRONTEND_URL` (custom)

Pour ajouter d'autres origines, modifiez `main.py` :

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "https://votre-domaine.com",
    ],
    ...
)
```

---

## üìä Performance

### Timeouts
- Requ√™tes HTTP vers Mistral : 60 secondes
- Streaming : pas de timeout (jusqu'√† `[DONE]`)

### Rate Limiting
G√©r√© par Mistral AI (voir [documentation](https://docs.mistral.ai/))

---

## üÜò Troubleshooting

### Le backend ne d√©marre pas
```bash
# V√©rifier les d√©pendances
pip install -r requirements.txt

# V√©rifier la cl√© API
echo $MISTRAL_API_KEY
```

### Erreurs 500
```bash
# Voir les logs d√©taill√©s
uvicorn main:app --reload --log-level debug
```

### Frontend ne re√ßoit pas de r√©ponse
- V√©rifier CORS (voir logs)
- V√©rifier que `OLLAMA_URL` pointe vers le bon backend
- Tester l'endpoint directement avec `curl`
